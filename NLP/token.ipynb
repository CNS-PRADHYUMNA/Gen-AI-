{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a57c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "corpus = \"\"\"Hello there! How are you doing today? This is a test.\"\"\"\n",
    "sentences = sent_tokenize(corpus, language='english')\n",
    "print(sentences)\n",
    "\n",
    "# below is the output of the above code\n",
    "# ['Hello there!', 'How are you doing today?', 'This is a test.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(corpus, language='english')\n",
    "# below is the output of the above code\n",
    "# ['Hello', 'there', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'This', 'is', 'a', 'test', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "wordpunct_tokenize(corpus, language='english')\n",
    "# below is the output of the above code\n",
    "# ['Hello', 'there', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'This', 'is', 'a', 'test', '.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081efc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordDetokenizer\n",
    "\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "detokenized_sentence = detokenizer.detokenize(word_tokenize(corpus, language='english'))\n",
    "print(detokenized_sentence)\n",
    "\n",
    "# below is the output of the above code\n",
    "# Hello there! How are you doing today? This is a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5744fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats -> eat\n",
      "eating -> eat\n",
      "eaten -> eaten\n",
      "eat -> eat\n",
      "eater -> eater\n",
      "eatable -> eatabl\n",
      "programming -> program\n",
      "programmer -> programm\n",
      "programs -> program\n"
     ]
    }
   ],
   "source": [
    "# Stemming \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words=['eats', 'eating', 'eaten', 'eat', 'eater', 'eatable','programming', 'programmer', 'programs']\n",
    "for word in words:\n",
    "    print(f\"{word} -> {stemmer.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats -> eat\n",
      "eating -> eat\n",
      "eaten -> eaten\n",
      "eat -> eat\n",
      "eater -> eater\n",
      "eatable -> eat\n",
      "programming -> programm\n",
      "programmer -> programmer\n",
      "programs -> program\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4) # This will stem words ending with 'ing', 's', 'e', or 'able' if they are at least 4 characters long\n",
    "words=['eats', 'eating', 'eaten', 'eat', 'eater', 'eatable','programming', 'programmer', 'programs']\n",
    "for word in words:\n",
    "    print(f\"{word} -> {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9abba467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats -> eat\n",
      "eating -> eat\n",
      "eaten -> eaten\n",
      "eat -> eat\n",
      "eater -> eater\n",
      "eatable -> eatabl\n",
      "programming -> program\n",
      "programmer -> programm\n",
      "programs -> program\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "words=['eats', 'eating', 'eaten', 'eat', 'eater', 'eatable','programming', 'programmer', 'programs']\n",
    "for word in words:\n",
    "    print(f\"{word} -> {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a46ed9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a8271b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats -> eat\n",
      "eating -> eat\n",
      "eaten -> eat\n",
      "eat -> eat\n",
      "eater -> eater\n",
      "eatable -> eatable\n",
      "programming -> program\n",
      "programmer -> programmer\n",
      "programs -> program\n",
      "betss -> betss\n",
      "better -> better\n",
      "best -> best\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words=['eats', 'eating', 'eaten', 'eat', 'eater', 'eatable','programming', 'programmer', 'programs',\"betss\",\"better\",\"best\"]\n",
    "for word in words:\n",
    "    print(f\"{word} -> {lemmatizer.lemmatize(word, pos='v')}\")  # 'v' for verb, you can change it to 'n' for noun, 'a' for adjective, etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab1f6515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "print(len(stop_words))  # Number of stopwords in English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "286f4e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comprehensive course walks\n",
      "fundamentals Generative AI\n",
      "Building advanced AI applications Langchain\n",
      "Seamless integration fine-tuning Huggingface models\n",
      "Real-world deployment strategies (Cloud & On-premise)\n",
      "Developing Retrieval-Augmented Generation (RAG) pipelines\n",
      "Full lifecycle training optimization\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"This comprehensive course walks you through\n",
    "The fundamentals of Generative AI\n",
    "Building advanced AI applications with Langchain\n",
    "Seamless integration and fine-tuning of Huggingface models\n",
    "Real-world deployment strategies (Cloud & On-premise)\n",
    "Developing Retrieval-Augmented Generation (RAG) pipelines\n",
    "Full lifecycle from training to optimization\"\"\"\n",
    "\n",
    "sentences=text.split('\\n')\n",
    "\n",
    "for sentence in sentences:\n",
    "    words= lemmatizer.lemmatize(sentence, pos='v').split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    print(' '.join(filtered_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be384ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = ['It was a great experience', 'I did not like the service', 'The product is amazing', 'I am disappointed with the quality' , 'The food was delicious', 'I am not satisfied with the purchase']\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
